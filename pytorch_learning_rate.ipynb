{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数、优化器和学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个简单的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (emb): Embedding(5, 5)\n",
       "  (linear): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (activate): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(5, 5)\n",
    "        self.linear = torch.nn.Linear(5, 3)\n",
    "        self.linear2 = torch.nn.Linear(3, 1)\n",
    "        self.activate = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activate(self.linear2(self.linear(self.emb(x))))\n",
    "\n",
    "    \n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型的参数，对应Parameter类，它有两个属性data（一个tensor，require_grad为False）和require_grad（一般为True）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你直接打印一个parameter，它的可读性很差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.8740, -0.3672, -0.4662, -0.8098,  0.8192],\n",
      "        [ 0.4842,  0.1593,  1.7196, -0.8103, -0.5121],\n",
      "        [-0.4298,  0.4086, -0.9216,  0.0504, -0.4167],\n",
      "        [-0.7561,  0.3412, -0.6166,  0.0970, -1.5422],\n",
      "        [ 0.6535, -1.4815, -0.0965,  1.2927, -2.1822]], requires_grad=True)\n",
      "====================================================================================================\n",
      "tensor([[-0.8740, -0.3672, -0.4662, -0.8098,  0.8192],\n",
      "        [ 0.4842,  0.1593,  1.7196, -0.8103, -0.5121],\n",
      "        [-0.4298,  0.4086, -0.9216,  0.0504, -0.4167],\n",
      "        [-0.7561,  0.3412, -0.6166,  0.0970, -1.5422],\n",
      "        [ 0.6535, -1.4815, -0.0965,  1.2927, -2.1822]])\n"
     ]
    }
   ],
   "source": [
    "print(model.emb.weight)\n",
    "print('='*100)\n",
    "print(model.emb.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "tensor([[-0.8740, -0.3672, -0.4662, -0.8098,  0.8192],\n",
      "        [ 0.4842,  0.1593,  1.7196, -0.8103, -0.5121],\n",
      "        [-0.4298,  0.4086, -0.9216,  0.0504, -0.4167],\n",
      "        [-0.7561,  0.3412, -0.6166,  0.0970, -1.5422],\n",
      "        [ 0.6535, -1.4815, -0.0965,  1.2927, -2.1822]])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "tensor([[-0.3699,  0.0258, -0.1693,  0.3095,  0.1445],\n",
      "        [ 0.3925,  0.2926,  0.2735,  0.0304, -0.3556],\n",
      "        [-0.3941, -0.3619,  0.0924, -0.1159, -0.3334]])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "tensor([-0.0530,  0.1841,  0.3760])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "tensor([[-0.2970, -0.4838,  0.1538]])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "tensor([-0.3113])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(type(p))\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight torch.Size([5, 5])\n",
      "linear.weight torch.Size([3, 5])\n",
      "linear.bias torch.Size([3])\n",
      "linear2.weight torch.Size([1, 3])\n",
      "linear2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer把参数分成若干个param_group，然后对他们进行更新。一般情况下，如果有模型所有参数的学习率相同，那么一个param_group就够了。\n",
    "\n",
    "在构建优化器optimizer时，需要传递一个列表过去，列表中的每个元素可以是一个parameter或者是一个param_group，param_group就是一个词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.8740, -0.3672, -0.4662, -0.8098,  0.8192],\n",
       "           [ 0.4842,  0.1593,  1.7196, -0.8103, -0.5121],\n",
       "           [-0.4298,  0.4086, -0.9216,  0.0504, -0.4167],\n",
       "           [-0.7561,  0.3412, -0.6166,  0.0970, -1.5422],\n",
       "           [ 0.6535, -1.4815, -0.0965,  1.2927, -2.1822]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.3699,  0.0258, -0.1693,  0.3095,  0.1445],\n",
       "           [ 0.3925,  0.2926,  0.2735,  0.0304, -0.3556],\n",
       "           [-0.3941, -0.3619,  0.0924, -0.1159, -0.3334]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0530,  0.1841,  0.3760], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.2970, -0.4838,  0.1538]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.3113], requires_grad=True)],\n",
       "  'lr': 0.001,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，param_group的键值对有：\n",
    "- params(list): 当前param_group中所有parameter\n",
    "- lr(float)：当前param_group的学习率\n",
    "- betas, weight_decay, amsgrad都是adam的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "控制学习率：根据参数的名字，对参数进行分组，缺失lr的param_group会直接使用全局学习率，也就是optimizer构造方法的第二个位置参数；\n",
    "\n",
    "然后在不同的epoch改变学习率的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.2970, -0.4838,  0.1538]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.3113], requires_grad=True)],\n",
       "  'lr': 0.1,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False},\n",
       " {'params': [Parameter containing:\n",
       "   tensor([[-0.8740, -0.3672, -0.4662, -0.8098,  0.8192],\n",
       "           [ 0.4842,  0.1593,  1.7196, -0.8103, -0.5121],\n",
       "           [-0.4298,  0.4086, -0.9216,  0.0504, -0.4167],\n",
       "           [-0.7561,  0.3412, -0.6166,  0.0970, -1.5422],\n",
       "           [ 0.6535, -1.4815, -0.0965,  1.2927, -2.1822]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.3699,  0.0258, -0.1693,  0.3095,  0.1445],\n",
       "           [ 0.3925,  0.2926,  0.2735,  0.0304, -0.3556],\n",
       "           [-0.3941, -0.3619,  0.0924, -0.1159, -0.3334]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0530,  0.1841,  0.3760], requires_grad=True)],\n",
       "  'lr': 0.01,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_param_groups = [{\"params\": [], \"lr\": 0.1}, {\"params\": [], \"lr\": 0.01}]\n",
    "for k, v in model.named_parameters():\n",
    "    if k.startswith('linear2'):\n",
    "        custom_param_groups[0][\"params\"].append(v)\n",
    "    else:\n",
    "        custom_param_groups[1][\"params\"].append(v)\n",
    "optimizer2 = optim.Adam(custom_param_groups)\n",
    "optimizer2.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于此，便实现人工修改学习率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.5] *",
   "language": "python",
   "name": "conda-env-pytorch1.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
